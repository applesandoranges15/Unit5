{"cells":[{"cell_type":"markdown","metadata":{},"source":["## SQL at Scale with Spark SQL\n","\n","Welcome to the SQL mini project. For this project, you will use the Databricks Platform and work through a series of exercises using Spark SQL. The dataset size may not be too big but the intent here is to familiarize yourself with the Spark SQL interface which scales easily to huge datasets, without you having to worry about changing your SQL queries. \n","\n","The data you need is present in the mini-project folder in the form of three CSV files. This data will be imported in Databricks to create the following tables under the __`country_club`__ database.\n","\n","<br>\n","1. The __`bookings`__ table,\n","2. The __`facilities`__ table, and\n","3. The __`members`__ table.\n","\n","You will be uploading these datasets shortly into the Databricks platform to understand how to create a database within minutes! Once the database and the tables are populated, you will be focusing on the mini-project questions.\n","\n","In the mini project, you'll be asked a series of questions. You can solve them using the databricks platform, but for the final deliverable,\n","please download this notebook as an IPython notebook (__`File -> Export -> IPython Notebook`__) and upload it to your GitHub."]},{"cell_type":"markdown","metadata":{},"source":["### Creating the Database\n","\n","We will first create our database in which we will be creating our three tables of interest"]},{"cell_type":"code","execution_count":117,"metadata":{},"outputs":[{"ename":"Py4JJavaError","evalue":"An error occurred while calling o24.sql.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.liftedTree1$1(InMemoryCatalog.scala:122)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.createDatabase(InMemoryCatalog.scala:119)\r\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createDatabase(ExternalCatalogWithListener.scala:47)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:281)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.createNamespace(V2SessionCatalog.scala:302)\r\n\tat org.apache.spark.sql.execution.datasources.v2.CreateNamespaceExec.run(CreateNamespaceExec.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[1;32mc:\\Users\\zoyaa\\Downloads\\mec-5.6.6-sql-with-spark-mini-project-20231102T223441Z-001\\mec-5.6.6-sql-with-spark-mini-project\\Mini_Project_SQL_with_Spark.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zoyaa/Downloads/mec-5.6.6-sql-with-spark-mini-project-20231102T223441Z-001/mec-5.6.6-sql-with-spark-mini-project/Mini_Project_SQL_with_Spark.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m spark\u001b[39m.\u001b[39msql(\u001b[39m\"\u001b[39m\u001b[39mDROP DATABASE IF EXISTS country_club CASCADE\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zoyaa/Downloads/mec-5.6.6-sql-with-spark-mini-project-20231102T223441Z-001/mec-5.6.6-sql-with-spark-mini-project/Mini_Project_SQL_with_Spark.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Create the database\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/zoyaa/Downloads/mec-5.6.6-sql-with-spark-mini-project-20231102T223441Z-001/mec-5.6.6-sql-with-spark-mini-project/Mini_Project_SQL_with_Spark.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m spark\u001b[39m.\u001b[39;49msql(\u001b[39m\"\u001b[39;49m\u001b[39mCREATE DATABASE country_club\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zoyaa/Downloads/mec-5.6.6-sql-with-spark-mini-project-20231102T223441Z-001/mec-5.6.6-sql-with-spark-mini-project/Mini_Project_SQL_with_Spark.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Show databases\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zoyaa/Downloads/mec-5.6.6-sql-with-spark-mini-project-20231102T223441Z-001/mec-5.6.6-sql-with-spark-mini-project/Mini_Project_SQL_with_Spark.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m spark\u001b[39m.\u001b[39msql(\u001b[39m\"\u001b[39m\u001b[39mSHOW DATABASES\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mshow()\n","File \u001b[1;32mc:\\Users\\zoyaa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[0;32m   1627\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m         litArgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonUtils\u001b[39m.\u001b[39mtoArray(\n\u001b[0;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m (args \u001b[39mor\u001b[39;00m [])]\n\u001b[0;32m   1630\u001b[0m         )\n\u001b[1;32m-> 1631\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsparkSession\u001b[39m.\u001b[39;49msql(sqlQuery, litArgs), \u001b[39mself\u001b[39m)\n\u001b[0;32m   1632\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1633\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(kwargs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n","File \u001b[1;32mc:\\Users\\zoyaa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n","File \u001b[1;32mc:\\Users\\zoyaa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n","File \u001b[1;32mc:\\Users\\zoyaa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n","\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o24.sql.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.liftedTree1$1(InMemoryCatalog.scala:122)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.createDatabase(InMemoryCatalog.scala:119)\r\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createDatabase(ExternalCatalogWithListener.scala:47)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:281)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.createNamespace(V2SessionCatalog.scala:302)\r\n\tat org.apache.spark.sql.execution.datasources.v2.CreateNamespaceExec.run(CreateNamespaceExec.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"]}],"source":["import os\n","from pyspark.sql import SparkSession\n","\n","# Set HADOOP_HOME\n","os.environ['HADOOP_HOME'] = '/path/to/your/hadoop'\n","\n","# Set hadoop.home.dir\n","os.environ['hadoop.home.dir'] = '/path/to/your/hadoop'\n","\n","# Create a Spark session\n","spark = SparkSession.builder.appName(\"mini-project\").getOrCreate()\n","\n","# Drop all tables in the database\n","spark.sql(\"DROP TABLE IF EXISTS country_club.bookings\")\n","spark.sql(\"DROP TABLE IF EXISTS country_club.facilities\")\n","spark.sql(\"DROP TABLE IF EXISTS country_club.members\")\n","\n","# Drop the database\n","spark.sql(\"DROP DATABASE IF EXISTS country_club CASCADE\")\n","\n","# Create the database\n","spark.sql(\"CREATE DATABASE country_club\")\n","\n","# Show databases\n","spark.sql(\"SHOW DATABASES\").show()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Creating the Tables\n","\n","In this section, we will be creating the three tables of interest and populate them with the data from the CSV files already available to you. \n","To get started, first upload the three CSV files to the DBFS as depicted in the following figure\n","\n","![](https://i.imgur.com/QcCruBr.png)\n","\n","\n","Once you have done this, please remember to execute the following code to build the dataframes which will be saved as tables in our database"]},{"cell_type":"code","execution_count":118,"metadata":{},"outputs":[],"source":["import shutil\n","\n","# Update these paths with your local paths\n","local_file_location_bookings = \"C:/Users/zoyaa/Downloads/mec-5.6.6-sql-with-spark-mini-project-20231102T223441Z-001/mec-5.6.6-sql-with-spark-mini-project/Bookings.csv\"\n","local_file_location_facilities = \"C:/Users/zoyaa/Downloads/mec-5.6.6-sql-with-spark-mini-project-20231102T223441Z-001/mec-5.6.6-sql-with-spark-mini-project/Facilities.csv\"\n","local_file_location_members = \"C:/Users/zoyaa/Downloads/mec-5.6.6-sql-with-spark-mini-project-20231102T223441Z-001/mec-5.6.6-sql-with-spark-mini-project/Members.csv\"\n","\n","# Use the local file paths in your Spark code\n","file_location_bookings = local_file_location_bookings\n","file_location_facilities = local_file_location_facilities\n","file_location_members = local_file_location_members\n","\n","# File type\n","file_type = \"csv\"\n","\n","# CSV options\n","infer_schema = \"true\"\n","first_row_is_header = \"true\"\n","delimiter = \",\"\n","\n","# The applied options are for CSV files. For other file types, these will be ignored.\n","bookings_df = (spark.read.format(file_type) \n","                    .option(\"inferSchema\", infer_schema) \n","                    .option(\"header\", first_row_is_header) \n","                    .option(\"sep\", delimiter) \n","                    .load(file_location_bookings))\n","\n","facilities_df = (spark.read.format(file_type) \n","                      .option(\"inferSchema\", infer_schema) \n","                      .option(\"header\", first_row_is_header) \n","                      .option(\"sep\", delimiter) \n","                      .load(file_location_facilities))\n","\n","members_df = (spark.read.format(file_type) \n","                      .option(\"inferSchema\", infer_schema) \n","                      .option(\"header\", first_row_is_header) \n","                      .option(\"sep\", delimiter) \n","                      .load(file_location_members))\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Viewing the dataframe schemas\n","\n","We can take a look at the schemas of our potential tables to be written to our database soon"]},{"cell_type":"code","execution_count":119,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Bookings Schema\n","root\n"," |-- bookid: integer (nullable = true)\n"," |-- facid: integer (nullable = true)\n"," |-- memid: integer (nullable = true)\n"," |-- starttime: timestamp (nullable = true)\n"," |-- slots: integer (nullable = true)\n","\n","Facilities Schema\n","root\n"," |-- facid: integer (nullable = true)\n"," |-- name: string (nullable = true)\n"," |-- membercost: double (nullable = true)\n"," |-- guestcost: double (nullable = true)\n"," |-- initialoutlay: integer (nullable = true)\n"," |-- monthlymaintenance: integer (nullable = true)\n","\n","Members Schema\n","root\n"," |-- memid: integer (nullable = true)\n"," |-- surname: string (nullable = true)\n"," |-- firstname: string (nullable = true)\n"," |-- address: string (nullable = true)\n"," |-- zipcode: integer (nullable = true)\n"," |-- telephone: string (nullable = true)\n"," |-- recommendedby: integer (nullable = true)\n"," |-- joindate: timestamp (nullable = true)\n","\n"]}],"source":["print('Bookings Schema')\n","bookings_df.printSchema()\n","print('Facilities Schema')\n","facilities_df.printSchema()\n","print('Members Schema')\n","members_df.printSchema()"]},{"cell_type":"markdown","metadata":{},"source":["### Create permanent tables\n","We will be creating three permanent tables here in our __`country_club`__ database as we discussed previously with the following code"]},{"cell_type":"code","execution_count":120,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+-----+-----+-------------------+-----+\n","|bookid|facid|memid|          starttime|slots|\n","+------+-----+-----+-------------------+-----+\n","|     0|    3|    1|2012-07-03 11:00:00|    2|\n","|     1|    4|    1|2012-07-03 08:00:00|    2|\n","|     3|    7|    1|2012-07-03 19:00:00|    2|\n","|     4|    8|    1|2012-07-03 10:00:00|    1|\n","|     5|    8|    1|2012-07-03 15:00:00|    1|\n","|    13|    6|    1|2012-07-04 15:30:00|    2|\n","|    18|    2|    1|2012-07-05 09:30:00|    3|\n","|    20|    3|    1|2012-07-05 19:00:00|    2|\n","|    23|    6|    1|2012-07-05 14:30:00|    2|\n","|    29|    2|    1|2012-07-06 17:00:00|    3|\n","|    30|    3|    1|2012-07-06 11:00:00|    2|\n","|    32|    6|    1|2012-07-06 14:00:00|    2|\n","|    40|    2|    1|2012-07-07 09:00:00|    3|\n","|    41|    2|    1|2012-07-07 11:30:00|    3|\n","|    42|    2|    1|2012-07-07 16:00:00|    3|\n","|    48|    6|    1|2012-07-07 10:30:00|    2|\n","|    49|    6|    1|2012-07-07 14:30:00|    2|\n","|    56|    1|    1|2012-07-08 15:00:00|    3|\n","|    57|    1|    1|2012-07-08 17:30:00|    3|\n","|    58|    3|    1|2012-07-08 11:30:00|    2|\n","+------+-----+-----+-------------------+-----+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","# Initialize Spark session\n","spark = SparkSession.builder.appName(\"MiniProject\").getOrCreate()\n","\n","# Define file paths\n","bookings_path = r'C:\\Users\\zoyaa\\Downloads\\mec-5.6.6-sql-with-spark-mini-project-20231102T223441Z-001\\mec-5.6.6-sql-with-spark-mini-project\\Bookings.csv'\n","facilities_path = r'C:\\Users\\zoyaa\\Downloads\\mec-5.6.6-sql-with-spark-mini-project-20231102T223441Z-001\\mec-5.6.6-sql-with-spark-mini-project\\Facilities.csv'\n","members_path = r'C:\\Users\\zoyaa\\Downloads\\mec-5.6.6-sql-with-spark-mini-project-20231102T223441Z-001\\mec-5.6.6-sql-with-spark-mini-project\\Members.csv'\n","\n","# Load data into Spark DataFrames\n","bookings_df = spark.read.csv(bookings_path, header=True, inferSchema=True)\n","facilities_df = spark.read.csv(facilities_path, header=True, inferSchema=True)\n","members_df = spark.read.csv(members_path, header=True, inferSchema=True)\n","\n","# Create temporary views for Spark SQL\n","bookings_df.createOrReplaceTempView(\"bookings\")\n","facilities_df.createOrReplaceTempView(\"facilities\")\n","members_df.createOrReplaceTempView(\"members\")\n","\n","# Example SQL query\n","result = spark.sql(\"\"\"\n","    SELECT *\n","    FROM bookings\n","    WHERE memid = 1\n","\"\"\")\n","result.show()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Refresh tables and check them"]},{"cell_type":"code","execution_count":121,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+----------+-----------+\n","|namespace| tableName|isTemporary|\n","+---------+----------+-----------+\n","|         |  bookings|       true|\n","|         |facilities|       true|\n","|         |   members|       true|\n","+---------+----------+-----------+\n","\n"]}],"source":["spark.sql(\"CREATE DATABASE IF NOT EXISTS country_club\")\n","\n","# Use the database\n","spark.sql(\"USE country_club\")\n","\n","# Refresh tables\n","spark.sql(\"REFRESH TABLE bookings\")\n","spark.sql(\"REFRESH TABLE facilities\")\n","spark.sql(\"REFRESH TABLE members\")\n","\n","# Show tables\n","tables = spark.sql(\"SHOW TABLES\")\n","tables.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Test a sample SQL query\n","\n","__Note:__ You can use __`%sql`__ at the beginning of a cell and write SQL queries directly as seen in the following cell. Neat isn't it!"]},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Bookings DataFrame:\n","   bookid  facid  memid            starttime  slots\n","0       0      3      1  2012-07-03 11:00:00      2\n","1       1      4      1  2012-07-03 08:00:00      2\n","2       2      6      0  2012-07-03 18:00:00      2\n","3       3      7      1  2012-07-03 19:00:00      2\n","4       4      8      1  2012-07-03 10:00:00      1\n","\n","Facilities DataFrame:\n","   facid             name  membercost  guestcost  initialoutlay  \\\n","0      0   Tennis Court 1         5.0       25.0          10000   \n","1      1   Tennis Court 2         5.0       25.0           8000   \n","2      2  Badminton Court         0.0       15.5           4000   \n","3      3     Table Tennis         0.0        5.0            320   \n","4      4   Massage Room 1         9.9       80.0           4000   \n","\n","   monthlymaintenance  \n","0                 200  \n","1                 200  \n","2                  50  \n","3                  10  \n","4                3000  \n","\n","Members DataFrame:\n","   memid   surname firstname                       address  zipcode  \\\n","0      0     GUEST     GUEST                         GUEST        0   \n","1      1     Smith    Darren    8 Bloomsbury Close, Boston     4321   \n","2      2     Smith     Tracy  8 Bloomsbury Close, New York     4321   \n","3      3    Rownam       Tim        23 Highway Way, Boston    23423   \n","4      4  Joplette    Janice    20 Crossing Road, New York      234   \n","\n","        telephone  recommendedby             joindate  \n","0  (000) 000-0000            NaN  2012-07-01 00:00:00  \n","1    555-555-5555            NaN  2012-07-02 12:02:05  \n","2    555-555-5555            NaN  2012-07-02 12:08:23  \n","3  (844) 693-0723            NaN  2012-07-03 09:32:15  \n","4  (833) 942-4710            1.0  2012-07-03 10:25:05  \n"]}],"source":["import pandas as pd\n","\n","# Load CSV files into DataFrames\n","bookings_df = pd.read_csv(r'C:\\Users\\zoyaa\\Downloads\\mec-5.6.6-sql-with-spark-mini-project-20231102T223441Z-001\\mec-5.6.6-sql-with-spark-mini-project\\Bookings.csv')\n","facilities_df = pd.read_csv(r'C:\\Users\\zoyaa\\Downloads\\mec-5.6.6-sql-with-spark-mini-project-20231102T223441Z-001\\mec-5.6.6-sql-with-spark-mini-project\\Facilities.csv')\n","members_df = pd.read_csv(r'C:\\Users\\zoyaa\\Downloads\\mec-5.6.6-sql-with-spark-mini-project-20231102T223441Z-001\\mec-5.6.6-sql-with-spark-mini-project\\Members.csv')\n","\n","# Display the first few rows of each DataFrame to verify the data has been loaded\n","print(\"Bookings DataFrame:\")\n","print(bookings_df.head())\n","\n","print(\"\\nFacilities DataFrame:\")\n","print(facilities_df.head())\n","\n","print(\"\\nMembers DataFrame:\")\n","print(members_df.head())\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'sqlalchemy'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32mc:\\Users\\zoyaa\\Downloads\\mec-5.6.6-sql-with-spark-mini-project-20231102T223441Z-001\\mec-5.6.6-sql-with-spark-mini-project\\Mini_Project_SQL_with_Spark.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zoyaa/Downloads/mec-5.6.6-sql-with-spark-mini-project-20231102T223441Z-001/mec-5.6.6-sql-with-spark-mini-project/Mini_Project_SQL_with_Spark.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msqlalchemy\u001b[39;00m \u001b[39mimport\u001b[39;00m create_engine\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zoyaa/Downloads/mec-5.6.6-sql-with-spark-mini-project-20231102T223441Z-001/mec-5.6.6-sql-with-spark-mini-project/Mini_Project_SQL_with_Spark.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zoyaa/Downloads/mec-5.6.6-sql-with-spark-mini-project-20231102T223441Z-001/mec-5.6.6-sql-with-spark-mini-project/Mini_Project_SQL_with_Spark.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Replace 'your_database_url' with the actual URL of your database\u001b[39;00m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sqlalchemy'"]}],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### Q1: Some of the facilities charge a fee to members, but some do not. Please list the names of the facilities that do."]},{"cell_type":"code","execution_count":123,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"c:\\Users\\zoyaa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sql\\magic.py\", line 196, in execute\n","    conn = sql.connection.Connection.set(\n","  File \"c:\\Users\\zoyaa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sql\\connection.py\", line 82, in set\n","    raise ConnectionError(\n","sql.connection.ConnectionError: Environment variable $DATABASE_URL not set, and no connect string given.\n","\n","Connection info needed in SQLAlchemy format, example:\n","               postgresql://username:password@hostname/dbname\n","               or an existing connection: dict_keys([])\n"]},{"name":"stdout","output_type":"stream","text":["+--------------+\n","|          name|\n","+--------------+\n","|Tennis Court 1|\n","|Tennis Court 2|\n","|Massage Room 1|\n","|Massage Room 2|\n","|  Squash Court|\n","+--------------+\n","\n"]}],"source":["%sql\n","\n","\n","\n","# Assuming 'spark' is your SparkSession\n","result = spark.sql(\"SELECT name FROM facilities WHERE membercost > 0\")\n","result.show()\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["####  Q2: How many facilities do not charge a fee to members?"]},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: ipython-sql in c:\\users\\zoyaa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.5.0)The sql extension is already loaded. To reload it, use:\n","  %reload_ext sql\n","\n","Requirement already satisfied: prettytable in c:\\users\\zoyaa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython-sql) (3.9.0)\n","Requirement already satisfied: ipython in c:\\users\\zoyaa\\appdata\\roaming\\python\\python310\\site-packages (from ipython-sql) (8.17.2)\n","Requirement already satisfied: sqlalchemy>=2.0 in c:\\users\\zoyaa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython-sql) (2.0.23)\n","Requirement already satisfied: sqlparse in c:\\users\\zoyaa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython-sql) (0.4.3)\n","Requirement already satisfied: six in c:\\users\\zoyaa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython-sql) (1.16.0)\n","Requirement already satisfied: ipython-genutils in c:\\users\\zoyaa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython-sql) (0.2.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\zoyaa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sqlalchemy>=2.0->ipython-sql) (4.5.0)\n","Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\zoyaa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sqlalchemy>=2.0->ipython-sql) (3.0.1)\n","Requirement already satisfied: decorator in c:\\users\\zoyaa\\appdata\\roaming\\python\\python310\\site-packages (from ipython->ipython-sql) (5.1.1)\n","Requirement already satisfied: jedi>=0.16 in c:\\users\\zoyaa\\appdata\\roaming\\python\\python310\\site-packages (from ipython->ipython-sql) (0.19.1)\n","Requirement already satisfied: matplotlib-inline in c:\\users\\zoyaa\\appdata\\roaming\\python\\python310\\site-packages (from ipython->ipython-sql) (0.1.6)\n","Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\zoyaa\\appdata\\roaming\\python\\python310\\site-packages (from ipython->ipython-sql) (3.0.39)\n","Requirement already satisfied: pygments>=2.4.0 in c:\\users\\zoyaa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython->ipython-sql) (2.14.0)\n","Requirement already satisfied: stack-data in c:\\users\\zoyaa\\appdata\\roaming\\python\\python310\\site-packages (from ipython->ipython-sql) (0.6.3)\n","Requirement already satisfied: traitlets>=5 in c:\\users\\zoyaa\\appdata\\roaming\\python\\python310\\site-packages (from ipython->ipython-sql) (5.13.0)\n","Requirement already satisfied: exceptiongroup in c:\\users\\zoyaa\\appdata\\roaming\\python\\python310\\site-packages (from ipython->ipython-sql) (1.1.3)\n","Requirement already satisfied: colorama in c:\\users\\zoyaa\\appdata\\roaming\\python\\python310\\site-packages (from ipython->ipython-sql) (0.4.6)\n","Requirement already satisfied: wcwidth in c:\\users\\zoyaa\\appdata\\roaming\\python\\python310\\site-packages (from prettytable->ipython-sql) (0.2.9)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\zoyaa\\appdata\\roaming\\python\\python310\\site-packages (from jedi>=0.16->ipython->ipython-sql) (0.8.3)\n","Requirement already satisfied: executing>=1.2.0 in c:\\users\\zoyaa\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython->ipython-sql) (2.0.1)\n","Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\zoyaa\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython->ipython-sql) (2.4.1)\n","Requirement already satisfied: pure-eval in c:\\users\\zoyaa\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython->ipython-sql) (0.2.2)\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]},{"name":"stdout","output_type":"stream","text":["+---------------------+\n","|num_facilities_no_fee|\n","+---------------------+\n","|                    4|\n","+---------------------+\n","\n"]}],"source":["# Install ipython-sql\n","!pip install ipython-sql\n","\n","# Load the sql extension\n","%load_ext sql\n","\n","# Assuming 'spark' is your SparkSession\n","result = spark.sql(\"SELECT COUNT(*) AS num_facilities_no_fee FROM facilities WHERE membercost = 0\")\n","result.show()\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Q3: How can you produce a list of facilities that charge a fee to members, where the fee is less than 20% of the facility's monthly maintenance cost? \n","#### Return the facid, facility name, member cost, and monthly maintenance of the facilities in question."]},{"cell_type":"code","execution_count":125,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"c:\\Users\\zoyaa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sql\\magic.py\", line 196, in execute\n","    conn = sql.connection.Connection.set(\n","  File \"c:\\Users\\zoyaa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sql\\connection.py\", line 82, in set\n","    raise ConnectionError(\n","sql.connection.ConnectionError: Environment variable $DATABASE_URL not set, and no connect string given.\n","\n","Connection info needed in SQLAlchemy format, example:\n","               postgresql://username:password@hostname/dbname\n","               or an existing connection: dict_keys([])\n","The sql extension is already loaded. To reload it, use:\n","  %reload_ext sql\n","+-----+--------------+----------+------------------+\n","|facid| facility_name|membercost|monthlymaintenance|\n","+-----+--------------+----------+------------------+\n","|    0|Tennis Court 1|       5.0|               200|\n","|    1|Tennis Court 2|       5.0|               200|\n","|    4|Massage Room 1|       9.9|              3000|\n","|    5|Massage Room 2|       9.9|              3000|\n","|    6|  Squash Court|       3.5|                80|\n","+-----+--------------+----------+------------------+\n","\n"]}],"source":["%sql\n","\n","%load_ext sql\n","\n","# Assuming 'spark' is your SparkSession\n","result = spark.sql(\"\"\"\n","    SELECT facid, name AS facility_name, membercost, monthlymaintenance\n","    FROM facilities\n","    WHERE membercost > 0 AND membercost < 0.2 * monthlymaintenance\n","\"\"\")\n","result.show()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Q4: How can you retrieve the details of facilities with ID 1 and 5? Write the query without using the OR operator."]},{"cell_type":"code","execution_count":126,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"c:\\Users\\zoyaa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sql\\magic.py\", line 196, in execute\n","    conn = sql.connection.Connection.set(\n","  File \"c:\\Users\\zoyaa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sql\\connection.py\", line 82, in set\n","    raise ConnectionError(\n","sql.connection.ConnectionError: Environment variable $DATABASE_URL not set, and no connect string given.\n","\n","Connection info needed in SQLAlchemy format, example:\n","               postgresql://username:password@hostname/dbname\n","               or an existing connection: dict_keys([])\n"]}],"source":["%%sql\n","\n","SELECT *\n","FROM facilities\n","WHERE facid IN (1, 5);\n","\n","# Separate cell for Python code\n","\n","%load_ext sql\n","\n","# Assuming 'spark' is your SparkSession\n","result = spark.sql(\"\"\"\n","    SELECT *\n","    FROM facilities\n","    WHERE facid IN (1, 5)\n","\"\"\")\n","result.show()\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Q5: How can you produce a list of facilities, with each labelled as 'cheap' or 'expensive', depending on if their monthly maintenance cost is more than $100? \n","#### Return the name and monthly maintenance of the facilities in question."]},{"cell_type":"code","execution_count":127,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"c:\\Users\\zoyaa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sql\\magic.py\", line 196, in execute\n","    conn = sql.connection.Connection.set(\n","  File \"c:\\Users\\zoyaa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sql\\connection.py\", line 82, in set\n","    raise ConnectionError(\n","sql.connection.ConnectionError: Environment variable $DATABASE_URL not set, and no connect string given.\n","\n","Connection info needed in SQLAlchemy format, example:\n","               postgresql://username:password@hostname/dbname\n","               or an existing connection: dict_keys([])\n","+---------------+------------------+---------+\n","|  facility_name|monthlymaintenance|    label|\n","+---------------+------------------+---------+\n","| Tennis Court 1|               200|expensive|\n","| Tennis Court 2|               200|expensive|\n","|Badminton Court|                50|    cheap|\n","|   Table Tennis|                10|    cheap|\n","| Massage Room 1|              3000|expensive|\n","| Massage Room 2|              3000|expensive|\n","|   Squash Court|                80|    cheap|\n","|  Snooker Table|                15|    cheap|\n","|     Pool Table|                15|    cheap|\n","+---------------+------------------+---------+\n","\n"]}],"source":["%sql\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, when\n","\n","# Assuming 'spark' is your SparkSession\n","spark = SparkSession.builder.appName(\"facility_analysis\").getOrCreate()\n","\n","# Assuming 'facilities' is your DataFrame representing the 'facilities' table\n","facilities = spark.sql(\"SELECT * FROM facilities\")\n","\n","# Create a new DataFrame with the desired columns and the 'label' column\n","result = facilities.select(\n","    col(\"name\").alias(\"facility_name\"),\n","    col(\"monthlymaintenance\"),\n","    when(col(\"monthlymaintenance\") > 100, \"expensive\").otherwise(\"cheap\").alias(\"label\")\n",")\n","\n","# Show the result\n","result.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Q6: You'd like to get the first and last name of the last member(s) who signed up. Do not use the LIMIT clause for your solution."]},{"cell_type":"code","execution_count":128,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["   firstname surname\n","30    Darren   Smith\n"]}],"source":["import pandas as pd\n","\n","# Read the Members.csv file into a DataFrame\n","members_df = pd.read_csv(\"Members.csv\")\n","\n","# Convert 'joindate' column to datetime if needed\n","members_df['joindate'] = pd.to_datetime(members_df['joindate'])\n","\n","# Find the maximum join date\n","max_joindate = members_df['joindate'].max()\n","\n","# Filter members with the maximum join date\n","result = members_df[members_df['joindate'] == max_joindate][['firstname', 'surname']]\n","\n","# Display the result\n","print(result)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["####  Q7: How can you produce a list of all members who have used a tennis court?\n","- Include in your output the name of the court, and the name of the member formatted as a single column. \n","- Ensure no duplicate data\n","- Also order by the member name."]},{"cell_type":"code","execution_count":129,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["             member_name            name\n","1425     Bader, Florence  Tennis Court 2\n","1802     Bader, Florence  Tennis Court 1\n","1388         Baker, Anne  Tennis Court 2\n","1773         Baker, Anne  Tennis Court 1\n","1803      Baker, Timothy  Tennis Court 1\n","1433      Baker, Timothy  Tennis Court 2\n","1234         Boothe, Tim  Tennis Court 2\n","1726         Boothe, Tim  Tennis Court 1\n","1217     Butters, Gerald  Tennis Court 2\n","1613     Butters, Gerald  Tennis Court 1\n","1834        Coplin, Joan  Tennis Court 1\n","1865      Crumpet, Erica  Tennis Court 1\n","1223         Dare, Nancy  Tennis Court 2\n","1701         Dare, Nancy  Tennis Court 1\n","1855      Farrell, David  Tennis Court 1\n","1452      Farrell, David  Tennis Court 2\n","1423     Farrell, Jemima  Tennis Court 2\n","1779     Farrell, Jemima  Tennis Court 1\n","1069        GUEST, GUEST  Tennis Court 2\n","1458        GUEST, GUEST  Tennis Court 1\n","1833    Genting, Matthew  Tennis Court 1\n","1454          Hunt, John  Tennis Court 2\n","1861          Hunt, John  Tennis Court 1\n","1358        Jones, David  Tennis Court 2\n","1748        Jones, David  Tennis Court 1\n","1846      Jones, Douglas  Tennis Court 1\n","1594    Joplette, Janice  Tennis Court 1\n","1209    Joplette, Janice  Tennis Court 2\n","1317       Owen, Charles  Tennis Court 2\n","1731       Owen, Charles  Tennis Court 1\n","1817       Pinker, David  Tennis Court 1\n","1453  Purview, Millicent  Tennis Court 2\n","1203         Rownam, Tim  Tennis Court 2\n","1588         Rownam, Tim  Tennis Court 1\n","1451   Rumney, Henrietta  Tennis Court 2\n","1841   Sarwin, Ramnaresh  Tennis Court 1\n","1440   Sarwin, Ramnaresh  Tennis Court 2\n","1182       Smith, Darren  Tennis Court 2\n","1424         Smith, Jack  Tennis Court 2\n","1780         Smith, Jack  Tennis Court 1\n","1558        Smith, Tracy  Tennis Court 1\n","1201        Smith, Tracy  Tennis Court 2\n","1286    Stibbons, Ponder  Tennis Court 2\n","1730    Stibbons, Ponder  Tennis Court 1\n","1670       Tracy, Burton  Tennis Court 1\n","1220       Tracy, Burton  Tennis Court 2\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\zoyaa\\AppData\\Local\\Temp\\ipykernel_15332\\2263571750.py:15: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  tennis_court_bookings['member_name'] = tennis_court_bookings['surname'] + ', ' + tennis_court_bookings['firstname']\n"]}],"source":["import pandas as pd\n","\n","# Load Members.csv, Bookings.csv, and Facilities.csv into Pandas DataFrames\n","members_df = pd.read_csv(\"Members.csv\")\n","bookings_df = pd.read_csv(\"Bookings.csv\")\n","facilities_df = pd.read_csv(\"Facilities.csv\")\n","\n","# Merge the Members, Bookings, and Facilities DataFrames\n","merged_df = pd.merge(pd.merge(members_df, bookings_df, on='memid'), facilities_df, on='facid')\n","\n","# Filter for tennis court bookings\n","tennis_court_bookings = merged_df[merged_df['name'].str.startswith('Tennis Court')]\n","\n","# Create a formatted column for member names\n","tennis_court_bookings['member_name'] = tennis_court_bookings['surname'] + ', ' + tennis_court_bookings['firstname']\n","\n","# Select and display the desired columns\n","result = tennis_court_bookings[['member_name', 'name']].drop_duplicates().sort_values('member_name')\n","print(result)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Q8: How can you produce a list of bookings on the day of 2012-09-14 which will cost the member (or guest) more than $30? \n","\n","- Remember that guests have different costs to members (the listed costs are per half-hour 'slot')\n","- The guest user's ID is always 0. \n","\n","#### Include in your output the name of the facility, the name of the member formatted as a single column, and the cost.\n","\n","- Order by descending cost, and do not use any subqueries."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["result = run_sql('''\n","                    SELECT name,\n","                           CONCAT(firstname,' ',surname) AS member_name,\n","                           CASE WHEN bookings.memid = 0 THEN guestcost * slots\n","                           ELSE membercost * slots END AS cost \n","                    FROM bookings\n","                    JOIN facilities\n","                      ON Bookings.facid = facilities.facid\n","                    JOIN members\n","                      ON bookings.memid = members.memid\n","                    WHERE starttime LIKE '%%2012-09-14%%'AND\n","                          ((guestcost * slots > 30 AND bookings.memid = 0) OR\n","                          (membercost * slots > 30 AND bookings.memid != 0))\n","                    ORDER BY cost DESC\n","                 ''')\n","result.toPandas()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Q9: This time, produce the same result as in Q8, but using a subquery."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%sql\n","\n","result = run_sql('''\n","                    SELECT DISTINCT name, surname, \n","                            CASE\n","                               WHEN surname == 'GUEST' THEN guestcost\n","                               ELSE membercost\n","                            END as cost \n","                    FROM (SELECT *\n","                            FROM bookings \n","                            INNER JOIN facilities\n","                            USING (facid)\n","                            INNER JOIN members\n","                            USING (memid))\n","                    WHERE starttime like \"%2012-09-14%\"\n","                   AND (membercost > 30\n","                   OR guestcost > 30)\n","                   ORDER BY cost desc\n","                 ''')\n","result.toPandas()\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Q10: Produce a list of facilities with a total revenue less than 1000.\n","- The output should have facility name and total revenue, sorted by revenue. \n","- Remember that there's a different cost for guests and members!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["result = run_sql('''\n","                    SELECT name,\n","                           SUM(fee * slots) AS revenue\n","                    FROM (SELECT CASE WHEN bookings.memid = 0 THEN guestcost\n","                                 ELSE membercost END AS fee,\n","                                 slots,\n","                                 name\n","                          FROM bookings\n","                          JOIN facilities\n","                            ON bookings.facid = facilities.facid) AS booking_Fees\n","                    GROUP BY name\n","                    HAVING revenue < 1000\n","                    ORDER BY revenue\n","                 ''')\n","result.toPandas()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"name":"Mini_Project_SQL_with_Spark","notebookId":1931807081501742},"nbformat":4,"nbformat_minor":0}
